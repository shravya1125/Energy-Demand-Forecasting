{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Energy Demand Forecasting \u2014 PJM (AEP region)\n\nAuthor: Your Name  \nInternship ID: INTERNSHIP_17513641056863b20937d78  \nTheme: Sustainable Energy & Efficiency\n\nThis notebook is pre-configured for `AEP_hourly.csv` (PJM dataset). It trains\nLinear Regression, Random Forest and an optional LSTM model for forecasting.\n\n----\n**Notes before running:**\n- Put `AEP_hourly.csv` in the same folder as this notebook (or provide the path below).  \n- LSTM is optional \u2014 it will run only if `tensorflow` is available.  \n- For fast results during the internship demo, the notebook uses a recent slice\n  of the data for LSTM training (configurable).  \n- Keep the final `.ipynb` size < 10MB by avoiding large saved model files in the notebook itself.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nimport joblib\n\ndef mape(y_true, y_pred):\n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    denom = np.where(y_true==0, 1e-6, y_true)\n    return np.mean(np.abs((y_true - y_pred) / denom)) * 100\n\ndef rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CONFIG\nDATA_FILE = 'AEP_hourly.csv'  # update path if needed\nUSE_LSTM = True              # set False if tensorflow not installed or you want faster run\nLSTM_SEQ_HOURS = 48          # how many past hours to use for LSTM\nLSTM_EPOCHS = 8              # keep small for quick demo; increase if you want better fit\nLSTM_BATCH_SIZE = 64\nRECENT_YEARS_FOR_LSTM = 2    # use last N years for LSTM to speed up training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "assert os.path.exists(DATA_FILE), f\"File not found: {DATA_FILE}. Put AEP_hourly.csv in same folder or update DATA_FILE path.\"\ndf = pd.read_csv(DATA_FILE, parse_dates=['Datetime'])\ndf = df.rename(columns={'Datetime':'timestamp','MW':'energy'})\ndf = df.sort_values('timestamp').reset_index(drop=True)\nprint('Columns:', df.columns.tolist())\nprint('Dataset length:', len(df))\ndf.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.figure(figsize=(12,4))\nplt.plot(df['timestamp'].iloc[:24*14], df['energy'].iloc[:24*14])\nplt.title('Energy demand (first 14 days)')\nplt.xlabel('Time')\nplt.ylabel('Energy (MW)')\nplt.tight_layout()\nplt.show()\n\ndf_daily = df.set_index('timestamp').resample('D').mean().reset_index()\nplt.figure(figsize=(12,4))\nplt.plot(df_daily['timestamp'].iloc[:90], df_daily['energy'].iloc[:90])\nplt.title('Daily average energy (first 90 days)')\nplt.xlabel('Date')\nplt.ylabel('Energy (MW)')\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_fe = df.copy()\ndf_fe['hour'] = df_fe['timestamp'].dt.hour\ndf_fe['dayofweek'] = df_fe['timestamp'].dt.dayofweek\ndf_fe['month'] = df_fe['timestamp'].dt.month\nfor lag in [1,24,168]:\n    df_fe[f'lag_{lag}'] = df_fe['energy'].shift(lag)\ndf_fe['rmean_24'] = df_fe['energy'].rolling(window=24).mean().shift(1)\ndf_fe['rmean_168'] = df_fe['energy'].rolling(window=168).mean().shift(1)\ndf_fe = df_fe.dropna().reset_index(drop=True)\nprint('After feature engineering length:', len(df_fe))\ndf_fe.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "train_size = int(0.8 * len(df_fe))\ntrain = df_fe.iloc[:train_size].copy()\ntest = df_fe.iloc[train_size:].copy()\nfeatures = ['lag_1','lag_24','lag_168','rmean_24','rmean_168','hour','dayofweek','month']\ntarget = 'energy'\nX_train = train[features].values\ny_train = train[target].values\nX_test = test[features].values\ny_test = test[target].values\nprint('Train shape:', X_train.shape, 'Test shape:', X_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "scaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\nlr = LinearRegression()\nlr.fit(X_train_scaled, y_train)\npred_lr = lr.predict(X_test_scaled)\nprint('Linear Regression RMSE:', rmse(y_test, pred_lr))\nprint('Linear Regression MAPE:', mape(y_test, pred_lr))\nrf = RandomForestRegressor(n_estimators=150, max_depth=12, random_state=42, n_jobs=-1)\nrf.fit(X_train, y_train)\npred_rf = rf.predict(X_test)\nprint('Random Forest RMSE:', rmse(y_test, pred_rf))\nprint('Random Forest MAPE:', mape(y_test, pred_rf))\nplt.figure(figsize=(12,4))\nplt.plot(test['timestamp'].values, y_test, label='Actual', linewidth=1)\nplt.plot(test['timestamp'].values, pred_rf, label='RF Predicted', linewidth=1)\nplt.legend()\nplt.xlabel('Time')\nplt.ylabel('Energy (MW)')\nplt.title('Actual vs RF Predicted (Test set)')\nplt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "try:\n    import tensorflow as tf\n    from tensorflow.keras.models import Sequential\n    from tensorflow.keras.layers import LSTM, Dense, Dropout\n    TF_AVAILABLE = True\n    print('TensorFlow version:', tf.__version__)\nexcept Exception as e:\n    print('TensorFlow not available. LSTM will be skipped. Error:', e)\n    TF_AVAILABLE = False\n\nif USE_LSTM and TF_AVAILABLE:\n    df_lstm = df_fe.copy()\n    if RECENT_YEARS_FOR_LSTM is not None:\n        last_date = df_lstm['timestamp'].max()\n        cutoff = last_date - pd.DateOffset(years=RECENT_YEARS_FOR_LSTM)\n        df_lstm = df_lstm[df_lstm['timestamp'] >= cutoff].reset_index(drop=True)\n        print('Using recent slice for LSTM from', cutoff.date(), 'to', last_date.date(), 'length:', len(df_lstm))\n    X_all = df_lstm[features].values\n    y_all = df_lstm[target].values\n    scaler_lstm = StandardScaler()\n    X_all_scaled = scaler_lstm.fit_transform(X_all)\n    def create_sequences(X, y, seq_len=LSTM_SEQ_HOURS):\n        Xs, ys = [], []\n        for i in range(seq_len, len(X)):\n            Xs.append(X[i-seq_len:i])\n            ys.append(y[i])\n        return np.array(Xs), np.array(ys)\n    X_seq, y_seq = create_sequences(X_all_scaled, y_all, seq_len=LSTM_SEQ_HOURS)\n    split_idx = int(0.8 * len(X_seq))\n    X_seq_train, X_seq_test = X_seq[:split_idx], X_seq[split_idx:]\n    y_seq_train, y_seq_test = y_seq[:split_idx], y_seq[split_idx:]\n    print('LSTM seq shapes:', X_seq_train.shape, X_seq_test.shape)\n    model = Sequential()\n    model.add(LSTM(64, input_shape=(X_seq_train.shape[1], X_seq_train.shape[2])))\n    model.add(Dropout(0.2))\n    model.add(Dense(32, activation='relu'))\n    model.add(Dense(1))\n    model.compile(optimizer='adam', loss='mse')\n    history = model.fit(X_seq_train, y_seq_train, epochs=LSTM_EPOCHS, batch_size=LSTM_BATCH_SIZE, validation_data=(X_seq_test, y_seq_test))\n    pred_lstm = model.predict(X_seq_test).flatten()\n    print('LSTM RMSE:', rmse(y_seq_test, pred_lstm))\n    print('LSTM MAPE:', mape(y_seq_test, pred_lstm))\n    plt.figure(figsize=(8,4))\n    plt.plot(history.history['loss'], label='train_loss')\n    plt.plot(history.history['val_loss'], label='val_loss')\n    plt.title('LSTM Training Loss')\n    plt.legend()\n    plt.show()\nelse:\n    print('Skipping LSTM (USE_LSTM=False or TensorFlow not available)')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "os.makedirs('./artifacts', exist_ok=True)\njoblib.dump(scaler, './artifacts/scaler.joblib')\njoblib.dump(rf, './artifacts/random_forest.joblib')\nprint('Saved artifacts to ./artifacts')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion & Next steps\n- Replace dataset with other PJM region CSVs to compare regions.  \n- For production or larger experiments use cross-validation with time-series splits and hyperparameter tuning.  \n- If adding weather features, merge hourly weather by timestamp.\n\nEnd of notebook."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}